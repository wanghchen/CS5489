{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** \\_\\_\\_\\_\\_Huanchen Wang\\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_57558749\\_\\_\\_\\_\\_\n",
    "\n",
    "**Kaggle Team Name:** \\_\\_\\_\\_\\_unknownwang\\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Assignment 1 - Movie Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission\n",
    "In this file, put the code that generates your final Kaggle submission. It will be used to verify that your Kaggle submission is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T02:13:28.575367Z",
     "start_time": "2023-01-27T02:13:28.120053Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib_inline   # setup output image format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "import csv\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T02:13:28.579741Z",
     "start_time": "2023-01-27T02:13:28.576533Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text_data(fname):\n",
    "    txtdata = []\n",
    "    revid   = []\n",
    "    classes = []\n",
    "    with open(fname, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            # get the text\n",
    "            txtdata.append(row[0])\n",
    "            revid.append(row[1])\n",
    "            # get the class (convert to integer)\n",
    "            if len(row)>2:\n",
    "                classes.append(int(row[2]))\n",
    "    \n",
    "    if (len(classes)>0) and (len(txtdata) != len(revid)):        \n",
    "        warn.error(\"mismatched length!\")\n",
    "    \n",
    "    return (txtdata, revid, classes)\n",
    "\n",
    "def write_csv_kaggle_sub(fname, Y):\n",
    "    # fname = file name\n",
    "    # Y is a list/array with class entries\n",
    "    \n",
    "    # header\n",
    "    tmp = [['Id', 'Prediction']]\n",
    "    \n",
    "    # add ID numbers for each Y\n",
    "    for (i,y) in enumerate(Y):\n",
    "        tmp2 = [(i+1), y]\n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T02:13:28.674284Z",
     "start_time": "2023-01-27T02:13:28.580324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4006\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "(traintxt, trainrevid, trainY) = read_text_data(\"movierating_train.txt\")\n",
    "(testtxt, testrevid, _)        = read_text_data(\"movierating_test.txt\")\n",
    "\n",
    "print(len(traintxt))\n",
    "print(len(testtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "# nltk.download()\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(doc):\n",
    "    # get stopwords list\n",
    "    stop_words = stopwords.words('english')\n",
    "    # tokenize the text each word initially\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # remove the punctuation and stopwords in the text\n",
    "    tokens = [word for word  in tokens if word not in stop_words and word not in punctuation]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "traintxt = [tokenize_word(doc) for doc in traintxt]\n",
    "testtxt = [tokenize_word(doc) for doc in testtxt]\n",
    "\n",
    "dataset= datasets.Dataset.from_dict({'text':traintxt, 'label':trainY})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "import torch\n",
    "use_gpu = torch.cuda.is_available()\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\huancwang2/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\huancwang2/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\huancwang2/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\huancwang2/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\huancwang2/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae75b70f62e4f1c870b033231b352b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],padding=\"max_length\",truncation=True,max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\huancwang2/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\huancwang2/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",\n",
    "                                                            num_labels=4)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, seed=46, \n",
    "                                  lr_scheduler_type='cosine', logging_dir=\"test_trainer\", evaluation_strategy='epoch', metric_for_best_model='accuracy', greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "d:\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3605\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1353\n",
      "  Number of trainable parameters = 108313348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca38ab88d9874cecb5c8a33f2dc5ff99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cae52bf5dd4e0aa63447e0db1d83f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2587769031524658, 'eval_accuracy': 0.32418952618453867, 'eval_runtime': 5.7466, 'eval_samples_per_second': 69.78, 'eval_steps_per_second': 8.875, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer\\checkpoint-500\n",
      "Configuration saved in test_trainer\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3046, 'learning_rate': 3.152254249815226e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-500\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481bb7d5126a429780412fd725e6dc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9882444739341736, 'eval_accuracy': 0.5461346633416458, 'eval_runtime': 5.9257, 'eval_samples_per_second': 67.672, 'eval_steps_per_second': 8.607, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer\\checkpoint-1000\n",
      "Configuration saved in test_trainer\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0116, 'learning_rate': 1.3045084996304511e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e6e974e43646d6bc5f0e80dcaa9307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9391842484474182, 'eval_accuracy': 0.6034912718204489, 'eval_runtime': 6.1126, 'eval_samples_per_second': 65.602, 'eval_steps_per_second': 8.343, 'epoch': 3.0}\n",
      "{'train_runtime': 484.6037, 'train_samples_per_second': 22.317, 'train_steps_per_second': 2.792, 'train_loss': 1.0552321248643062, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1353, training_loss=1.0552321248643062, metrics={'train_runtime': 484.6037, 'train_samples_per_second': 22.317, 'train_steps_per_second': 2.792, 'train_loss': 1.0552321248643062, 'epoch': 3.0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ab0e30c47499ea0b0c63598523d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testdataset = datasets.Dataset.from_dict({'text':testtxt})\n",
    "\n",
    "test_tokenized_datasets = testdataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ee86e76f7045f2a8f876c6d61b5522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1,\n",
       "       0, 2, 2, 1, 0, 0, 3, 2, 2, 3, 2, 1, 2, 2, 0, 1, 2, 2, 2, 1, 2, 2,\n",
       "       0, 1, 3, 1, 2, 0, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 3, 0, 2, 1, 2, 2,\n",
       "       1, 2, 2, 1, 2, 2, 0, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 3, 0, 1, 2,\n",
       "       2, 3, 0, 1, 1, 1, 0, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2,\n",
       "       1, 1, 1, 2, 1, 3, 1, 2, 1, 1, 1, 2, 1, 3, 2, 3, 2, 2, 2, 2, 3, 1,\n",
       "       0, 2, 2, 3, 2, 2, 2, 2, 1, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 1, 1,\n",
       "       0, 3, 1, 1, 2, 1, 1, 0, 3, 2, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 2, 3,\n",
       "       1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1,\n",
       "       1, 1, 1, 0, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 0, 1, 2, 2,\n",
       "       2, 0, 1, 0, 3, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 2, 2, 2, 1, 2, 1,\n",
       "       2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 1, 2, 2, 3, 0, 2, 3, 3, 3, 2, 1, 2,\n",
       "       2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 3, 3, 2, 1, 2, 0, 1, 2, 1, 2, 2, 1,\n",
       "       1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 3, 2, 1, 1, 2, 2, 2, 2, 3, 2, 1,\n",
       "       2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 3, 2, 1, 1, 0, 2, 1, 2, 1, 0, 3,\n",
       "       2, 1, 3, 2, 2, 2, 2, 1, 1, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1,\n",
       "       0, 1, 2, 2, 1, 1, 2, 1, 1, 2, 0, 1, 0, 1, 2, 2, 0, 1, 3, 2, 1, 3,\n",
       "       1, 1, 1, 2, 2, 3, 2, 3, 0, 1, 2, 1, 2, 1, 3, 1, 2, 2, 1, 3, 3, 0,\n",
       "       0, 2, 1, 1, 2, 2, 2, 3, 0, 1, 1, 1, 1, 2, 2, 0, 0, 1, 1, 2, 1, 2,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 0,\n",
       "       1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 3, 3, 2, 2, 1, 1, 2,\n",
       "       0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 3, 3, 2, 1, 2, 2, 3, 0, 1, 2,\n",
       "       2, 2, 2, 3, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 0, 3, 2, 2, 1, 1, 2, 1,\n",
       "       2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 3, 2, 1, 0, 2, 2, 1, 2,\n",
       "       1, 1, 2, 1, 0, 1, 3, 2, 1, 2, 1, 3, 1, 3, 1, 2, 2, 0, 2, 1, 1, 2,\n",
       "       3, 2, 0, 1, 0, 0, 2, 2, 0, 2, 1, 1, 2, 3, 1, 2, 1, 1, 1, 2, 1, 2,\n",
       "       1, 2, 2, 2, 1, 0, 2, 0, 2, 1, 2, 1, 2, 3, 2, 0, 2, 1, 2, 1, 2, 1,\n",
       "       2, 1, 2, 1, 1, 1, 2, 2, 0, 1, 2, 0, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
       "       1, 2, 1, 0, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 0, 1, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2,\n",
       "       1, 2, 0, 2, 2, 1, 2, 1, 1, 1, 2, 1, 0, 2, 2, 1, 2, 2, 1, 2, 1, 3,\n",
       "       1, 3, 2, 1, 3, 0, 2, 3, 1, 2, 2, 2, 0, 1, 2, 2, 3, 2, 1, 2, 1, 2,\n",
       "       2, 3, 2, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 2, 0, 0, 1, 1, 1, 2, 2, 1,\n",
       "       3, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 3, 1, 0, 1, 3, 0, 0, 2,\n",
       "       2, 3, 2, 2, 0, 1, 0, 0, 1, 1, 1, 2, 1, 3, 0, 2, 2, 3, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 0, 2,\n",
       "       2, 1, 1, 2, 2, 2, 1, 3, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,\n",
       "       2, 2, 1, 2, 1, 1, 1, 0, 1, 3, 3, 1, 2, 0, 1, 3, 3, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 1, 2, 2, 3, 1,\n",
       "       2, 1, 2, 2, 1, 1, 3, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 3,\n",
       "       2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 1, 0, 2, 3, 2, 2, 2, 2, 3, 3, 2, 1,\n",
       "       2, 3, 0, 3, 2, 0, 2, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1,\n",
       "       0, 1, 1, 0, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2,\n",
       "       3, 2, 2, 2, 2, 1, 2, 1, 2, 1, 3, 3, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1,\n",
       "       3, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 3, 2, 3, 3, 2,\n",
       "       2, 2, 2, 2, 1, 1, 2, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = trainer.predict(test_tokenized_datasets).predictions.argmax(-1)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels.tolist()\n",
    "write_csv_kaggle_sub(\"my_submission.csv\", test_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
