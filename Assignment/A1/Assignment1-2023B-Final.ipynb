{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** \\_\\_\\_\\_\\_Huanchen Wang\\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_57558749\\_\\_\\_\\_\\_\n",
    "\n",
    "**Kaggle Team Name:** \\_\\_\\_\\_\\_unknownwang\\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Assignment 1 - Movie Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission\n",
    "In this file, put the code that generates your final Kaggle submission. It will be used to verify that your Kaggle submission is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T02:13:28.575367Z",
     "start_time": "2023-01-27T02:13:28.120053Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib_inline   # setup output image format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "import csv\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T02:13:28.579741Z",
     "start_time": "2023-01-27T02:13:28.576533Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text_data(fname):\n",
    "    txtdata = []\n",
    "    revid   = []\n",
    "    classes = []\n",
    "    with open(fname, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            # get the text\n",
    "            txtdata.append(row[0])\n",
    "            revid.append(row[1])\n",
    "            # get the class (convert to integer)\n",
    "            if len(row)>2:\n",
    "                classes.append(int(row[2]))\n",
    "    \n",
    "    if (len(classes)>0) and (len(txtdata) != len(revid)):        \n",
    "        warn.error(\"mismatched length!\")\n",
    "    \n",
    "    return (txtdata, revid, classes)\n",
    "\n",
    "def write_csv_kaggle_sub(fname, Y):\n",
    "    # fname = file name\n",
    "    # Y is a list/array with class entries\n",
    "    \n",
    "    # header\n",
    "    tmp = [['Id', 'Prediction']]\n",
    "    \n",
    "    # add ID numbers for each Y\n",
    "    for (i,y) in enumerate(Y):\n",
    "        tmp2 = [(i+1), y]\n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-27T02:13:28.674284Z",
     "start_time": "2023-01-27T02:13:28.580324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4006\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "(traintxt, trainrevid, trainY) = read_text_data(\"movierating_train.txt\")\n",
    "(testtxt, testrevid, _)        = read_text_data(\"movierating_test.txt\")\n",
    "\n",
    "print(len(traintxt))\n",
    "print(len(testtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "# nltk.download()\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(doc):\n",
    "    # get stopwords list\n",
    "    stop_words = stopwords.words('english')\n",
    "    # tokenize the text each word initially\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # remove the punctuation and stopwords in the text\n",
    "    tokens = [word for word  in tokens if word not in stop_words and word not in punctuation]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "traintxt = [tokenize_word(doc) for doc in traintxt]\n",
    "testtxt = [tokenize_word(doc) for doc in testtxt]\n",
    "\n",
    "dataset= datasets.Dataset.from_dict({'text':traintxt, 'label':trainY})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import torch\n",
    "use_gpu = torch.cuda.is_available()\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0164fe04c8345d6a4a3251605584556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"],padding=\"max_length\",truncation=True,max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",\n",
    "                                                            num_labels=4)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, seed=46, \n",
    "                                  lr_scheduler_type='cosine', logging_dir=\"test_trainer\", evaluation_strategy='epoch', metric_for_best_model='accuracy', greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "d:\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3605\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1353\n",
      "  Number of trainable parameters = 108313348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fb3290e238486d8e9b0ebd84ef8f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca95b5fbb45a473ca6a41b2a0c81aaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0684934854507446, 'eval_accuracy': 0.49625935162094764, 'eval_runtime': 5.3324, 'eval_samples_per_second': 75.201, 'eval_steps_per_second': 9.564, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer\\checkpoint-500\n",
      "Configuration saved in test_trainer\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2017, 'learning_rate': 1.3984473874891038e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-500\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80a8713c70a4016a4b2585e47ffb142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8920174837112427, 'eval_accuracy': 0.6109725685785536, 'eval_runtime': 5.4836, 'eval_samples_per_second': 73.127, 'eval_steps_per_second': 9.3, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer\\checkpoint-1000\n",
      "Configuration saved in test_trainer\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.826, 'learning_rate': 3.175206411937839e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc05f7c8145476d908a28a69af8fc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8893514275550842, 'eval_accuracy': 0.6209476309226932, 'eval_runtime': 5.5964, 'eval_samples_per_second': 71.654, 'eval_steps_per_second': 9.113, 'epoch': 3.0}\n",
      "{'train_runtime': 458.6118, 'train_samples_per_second': 23.582, 'train_steps_per_second': 2.95, 'train_loss': 0.9053627685185988, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1353, training_loss=0.9053627685185988, metrics={'train_runtime': 458.6118, 'train_samples_per_second': 23.582, 'train_steps_per_second': 2.95, 'train_loss': 0.9053627685185988, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c553b67ca240a9965964c929e936f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testdataset = datasets.Dataset.from_dict({'text':testtxt})\n",
    "\n",
    "test_tokenized_datasets = testdataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c5404883cd472ca656e1f51d58bd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 3, 2, 2, 3, 1, 1, 3, 1, 1, 1, 2, 3, 2, 1, 1, 2, 3, 3, 1,\n",
       "       0, 2, 3, 1, 1, 1, 3, 3, 2, 3, 1, 1, 2, 2, 1, 0, 2, 2, 2, 1, 1, 2,\n",
       "       0, 1, 3, 1, 1, 1, 1, 2, 2, 2, 1, 3, 1, 0, 1, 1, 3, 1, 3, 1, 3, 2,\n",
       "       0, 2, 2, 1, 3, 2, 0, 1, 3, 1, 1, 0, 1, 2, 0, 2, 2, 1, 3, 0, 1, 2,\n",
       "       2, 3, 0, 1, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 2, 2,\n",
       "       1, 2, 1, 2, 2, 3, 2, 2, 1, 1, 1, 2, 1, 3, 3, 2, 2, 2, 3, 2, 3, 2,\n",
       "       1, 1, 2, 3, 1, 3, 3, 1, 1, 0, 2, 2, 0, 2, 3, 2, 2, 3, 1, 2, 1, 1,\n",
       "       0, 3, 1, 1, 1, 1, 1, 0, 3, 2, 1, 0, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2,\n",
       "       1, 2, 0, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 3, 3, 1, 2, 2, 1, 2, 1,\n",
       "       1, 1, 0, 0, 3, 2, 1, 2, 1, 2, 1, 2, 3, 2, 2, 1, 1, 3, 0, 1, 2, 2,\n",
       "       2, 0, 1, 0, 3, 1, 1, 2, 1, 2, 2, 2, 2, 0, 2, 1, 3, 2, 2, 1, 1, 1,\n",
       "       3, 2, 3, 2, 2, 3, 1, 1, 2, 2, 1, 1, 3, 3, 0, 2, 3, 2, 3, 2, 1, 3,\n",
       "       3, 1, 2, 1, 2, 0, 1, 2, 1, 1, 3, 2, 2, 0, 2, 0, 1, 2, 2, 3, 2, 1,\n",
       "       1, 2, 0, 2, 2, 1, 2, 1, 2, 2, 1, 3, 2, 1, 1, 2, 2, 3, 2, 3, 3, 0,\n",
       "       2, 2, 1, 2, 3, 1, 1, 1, 1, 1, 2, 3, 3, 0, 2, 0, 3, 1, 2, 2, 0, 3,\n",
       "       2, 1, 3, 2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 3, 1, 2, 2, 2, 2, 1, 2, 1,\n",
       "       0, 1, 1, 2, 2, 0, 3, 0, 1, 3, 1, 1, 0, 1, 2, 3, 0, 1, 3, 2, 1, 3,\n",
       "       1, 1, 2, 2, 2, 3, 2, 3, 0, 1, 2, 1, 2, 2, 3, 1, 2, 2, 2, 3, 3, 0,\n",
       "       0, 2, 1, 1, 2, 3, 2, 3, 0, 2, 1, 2, 0, 2, 2, 0, 0, 2, 1, 1, 1, 3,\n",
       "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 0,\n",
       "       1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 3, 2, 2, 3, 1, 0, 3,\n",
       "       1, 1, 2, 3, 2, 2, 1, 3, 2, 1, 2, 0, 3, 3, 1, 1, 2, 2, 3, 0, 2, 1,\n",
       "       2, 3, 2, 3, 3, 1, 2, 3, 1, 1, 2, 1, 2, 2, 0, 3, 2, 2, 2, 1, 2, 1,\n",
       "       2, 2, 2, 2, 2, 0, 1, 3, 2, 2, 1, 2, 3, 2, 3, 2, 1, 0, 2, 2, 2, 2,\n",
       "       1, 2, 2, 1, 0, 1, 3, 2, 1, 2, 1, 3, 1, 3, 2, 2, 2, 0, 2, 1, 1, 2,\n",
       "       2, 1, 1, 2, 1, 0, 3, 2, 0, 2, 1, 1, 2, 3, 1, 3, 2, 2, 0, 2, 2, 2,\n",
       "       1, 2, 2, 2, 1, 1, 2, 0, 2, 2, 2, 1, 2, 2, 2, 0, 2, 1, 1, 2, 2, 2,\n",
       "       2, 0, 2, 1, 1, 1, 1, 2, 0, 1, 2, 0, 2, 1, 0, 3, 2, 1, 1, 2, 2, 1,\n",
       "       1, 2, 1, 1, 2, 3, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 1, 1, 3,\n",
       "       2, 2, 2, 3, 3, 0, 2, 1, 2, 1, 3, 0, 2, 3, 2, 0, 1, 2, 3, 2, 2, 3,\n",
       "       1, 2, 1, 3, 2, 2, 3, 1, 1, 1, 2, 2, 1, 0, 2, 1, 3, 2, 1, 3, 1, 3,\n",
       "       2, 3, 2, 1, 3, 1, 2, 3, 1, 2, 2, 1, 0, 1, 2, 2, 2, 3, 1, 2, 2, 2,\n",
       "       2, 3, 2, 1, 3, 2, 0, 2, 0, 1, 2, 1, 2, 2, 1, 0, 1, 1, 1, 2, 2, 1,\n",
       "       3, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 0, 2, 3, 1, 1, 1, 3, 1, 0, 2,\n",
       "       2, 3, 2, 1, 0, 2, 0, 0, 2, 2, 1, 2, 1, 3, 0, 2, 2, 3, 2, 2, 1, 1,\n",
       "       1, 1, 2, 0, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 0, 1, 1, 2,\n",
       "       2, 0, 1, 2, 1, 2, 1, 3, 2, 1, 1, 1, 2, 2, 2, 1, 2, 3, 1, 1, 2, 1,\n",
       "       2, 2, 1, 2, 1, 0, 2, 2, 1, 3, 3, 1, 2, 0, 1, 3, 3, 3, 3, 2, 3, 2,\n",
       "       1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 1, 1, 3, 1, 0, 1, 2, 1, 3, 3, 3, 2,\n",
       "       2, 1, 2, 2, 2, 2, 3, 1, 2, 3, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 3,\n",
       "       2, 2, 2, 2, 3, 2, 2, 1, 1, 0, 1, 0, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2,\n",
       "       2, 2, 0, 3, 2, 1, 2, 1, 3, 2, 1, 1, 0, 1, 2, 2, 1, 3, 1, 2, 2, 1,\n",
       "       0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 3,\n",
       "       3, 2, 2, 2, 2, 1, 2, 1, 2, 1, 3, 3, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1,\n",
       "       3, 2, 2, 2, 1, 3, 2, 1, 2, 1, 1, 1, 2, 2, 2, 3, 1, 3, 3, 3, 3, 2,\n",
       "       2, 2, 2, 2, 1, 1, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = trainer.predict(test_tokenized_datasets).predictions.argmax(-1)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels.tolist()\n",
    "write_csv_kaggle_sub(\"my_submission.csv\", test_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
